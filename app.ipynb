{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b532c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting PDF's...\n",
      "Chunks are being created...\n",
      "Chunks being added into vector database...\n",
      "Loading Tokenizer...\n",
      "Loading Model...\n",
      "RAG Question Answering System\n",
      "Type 'quit', 'exit', or 'stop' to end the session\n",
      "--------------------------------------------------\n",
      "\n",
      "Answer: Hybrid modeling is a promising approach to capturing the\n",
      "\n",
      "\n",
      "Examples of hybrid modeling\n",
      "\n",
      "Answer: Hybrid modeling approaches combine the flexibility of machine learning (ML)\n",
      "with the rigor of process-based models (FPMs). ML models are able to be trained on\n",
      "a wide range of data, from continuous variables to categorical values, and they can\n",
      "be used to make predictions even when data are scarce. FPMs, on the other hand, are\n",
      "limited to systems for which there is a rich body of knowledge and mathematical\n",
      "formulation, such as\n",
      "Thank you for using the RAG system. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM  \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.disable(logging.CRITICAL)  # Disable ALL logging below CRITICAL\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "\n",
    "# Function to read the contents of PDFs\n",
    "def read_pdfs(dataset_path):\n",
    "    all_docs = []\n",
    "\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith('.pdf'): \n",
    "\n",
    "            file_path = os.path.join(dataset_path, file)\n",
    "            loader = PyPDFLoader(file_path, mode=\"single\")\n",
    "            docs = loader.load()\n",
    "\n",
    "            all_docs.append(docs[0]) \n",
    "            \n",
    "    return all_docs\n",
    "\n",
    "# Function to divide the extracted text into chunks\n",
    "def generate_chunks(all_docs):\n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(separator = \"\\n\",\n",
    "                                          chunk_size = 900, chunk_overlap = 100,\n",
    "                                          length_function = len)\n",
    "\n",
    "    chunks = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    return chunks\n",
    "    \n",
    "# Function to convert extract chunks into embeddings and store them in vector database\n",
    "def store_embeddings(chunks):\n",
    "    \n",
    "    embedding = HuggingFaceEmbeddings(model_name = \"NeuML/pubmedbert-base-embeddings\")\n",
    "    \n",
    "    # Setting up vector store\n",
    "    vstore = AstraDBVectorStore(embedding = embedding,\n",
    "                                collection_name = \"langchain_pdf_query\",\n",
    "                                api_endpoint = ASTRA_DB_API_ENDPOINT,\n",
    "                                token = ASTRA_DB_APPLICATION_TOKEN)\n",
    "\n",
    "    vstore.add_documents(chunks)\n",
    "    astra_vector_index = VectorStoreIndexWrapper(vectorstore = vstore)\n",
    "    \n",
    "    return vstore\n",
    "\n",
    "def inference_through_llm(vstore, query, tokenizer, model):\n",
    "    \n",
    "    # Find the similiar chunks from the database\n",
    "    searchDocs = vstore.similarity_search(query, k=3)\n",
    "    \n",
    "    # Create the prompt\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in searchDocs])\n",
    "    prompt = f\"\"\"Based on the following context, please answer the question. Answer the question in descriptive way \n",
    "                 atleast in 4-5 lines.\n",
    "                 Context: {context_text}\n",
    "                 Question: {query}\n",
    "                 Answer:\"\"\"\n",
    "    \n",
    "    # Generate answer\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens = 100, min_new_tokens = 50, do_sample = True,\n",
    "                            temperature = 0.7, top_p = 0.9, pad_token_id = tokenizer.eos_token_id,\n",
    "                            stop_strings = [\"\\n\\nQuestion:\", \"\\nQuestion:\", \"Question:\"],\n",
    "                            tokenizer = tokenizer)\n",
    "    \n",
    "    # Extract just the answer\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = full_response[len(prompt):].strip()\n",
    "    \n",
    "    stop_patterns = [\n",
    "    \"\\nContext:\",\n",
    "    \"\\nQuestion:\", \n",
    "    \"\\n\\nQuestion:\",\n",
    "    \"\\nQ:\",\n",
    "    \"Context:\",\n",
    "    \"Question:\",\n",
    "    \"\\n\\n\\n\"]\n",
    "\n",
    "    for pattern in stop_patterns:\n",
    "        if pattern in answer:\n",
    "            answer = answer.split(pattern)[0].strip()\n",
    "            break\n",
    "    \n",
    "    return answer\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Load your API secret keys\n",
    "    try:\n",
    "        load_dotenv()\n",
    "        ASTRA_DB_APPLICATION_TOKEN = os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"]\n",
    "        ASTRA_DB_ID = os.environ[\"ASTRA_DB_ID\"]\n",
    "        ASTRA_DB_API_ENDPOINT = os.environ[\"ASTRA_DB_API_ENDPOINT\"]\n",
    "        \n",
    "    except:\n",
    "        print(\"Mention your API keys\")\n",
    "        \n",
    "    # make Hub downloads resilient on slower links\n",
    "    os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"180\"\n",
    "    os.environ[\"HF_HUB_DOWNLOAD_RETRY\"]   = \"20\"\n",
    "    \n",
    "    # Read PDF's\n",
    "    dataset_path = r\"D:\\Intelligent QA AI\\research_docs\"\n",
    "    print(\"Extracting PDF's...\")\n",
    "    all_docs = read_pdfs(dataset_path)\n",
    "    \n",
    "    # Generate chunks\n",
    "    print(\"Chunks are being created...\")\n",
    "    chunks = generate_chunks(all_docs)\n",
    "    \n",
    "    # Add chunks into vector database\n",
    "    print(\"Chunks being added into vector database...\")\n",
    "    vstore = store_embeddings(chunks)\n",
    "    \n",
    "    # Load the model and tokenizer into memory\n",
    "    model_id  = \"TheBloke/PMC_LLAMA-7B-GPTQ\"\n",
    "    print(\"Loading Tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    os.makedirs(\"./model_offload\", exist_ok=True)\n",
    "    print(\"Loading Model...\")\n",
    "    model = AutoGPTQForCausalLM.from_quantized(model_id,\n",
    "                                               device_map=\"auto\",\n",
    "                                               max_memory={0: \"5GB\", \"cpu\": \"14GB\"},  # Adjust based on your system\n",
    "                                               offload_folder=\"./model_offload\", use_safetensors=True, trust_remote_code=True)\n",
    "    # Enter user query\n",
    "    print(\"RAG Question Answering System\")\n",
    "    print(\"Type 'quit', 'exit', or 'stop' to end the session\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nAsk your query: \").strip()\n",
    "\n",
    "        # Check for exit commands\n",
    "        if query.lower() in ['quit', 'exit', 'stop', 'q']:\n",
    "            print(\"Thank you for using the RAG system. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Skip empty queries\n",
    "        if not query:\n",
    "            print(\"Please enter a valid query.\")\n",
    "            continue\n",
    "\n",
    "        # Process the query\n",
    "        answer = inference_through_llm(vstore, query, tokenizer, model)\n",
    "        print(f\"\\nAnswer: {answer}\")\n",
    "    \n",
    "    await vstore.aclear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472dd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
