{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b532c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM  \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Load your API secret keys\n",
    "    try:\n",
    "        load_dotenv()\n",
    "        ASTRA_DB_APPLICATION_TOKEN = os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"]\n",
    "        ASTRA_DB_ID = os.environ[\"ASTRA_DB_ID\"]\n",
    "        ASTRA_DB_API_ENDPOINT = os.environ[\"ASTRA_DB_API_ENDPOINT\"]\\\n",
    "        \n",
    "    except:\n",
    "        print(\"Mention your API keys\")\n",
    "        \n",
    "    # make Hub downloads resilient on slower links\n",
    "    os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"180\"\n",
    "    os.environ[\"HF_HUB_DOWNLOAD_RETRY\"]   = \"20\"\n",
    "    \n",
    "    dataset_path = r\"D:\\Intelligent QA AI\\research_docs\"\n",
    "    all_docs = read_pdfs(dataset_path)\n",
    "    \n",
    "    chunks = generate_chunks(all_docs)\n",
    "    \n",
    "    store_embeddings(chunks)\n",
    "\n",
    "# Function to read the contents of PDFs\n",
    "def read_pdfs(dataset_path):\n",
    "    all_docs = []\n",
    "\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith('.pdf'): \n",
    "\n",
    "            file_path = os.path.join(dataset_path, file)\n",
    "            loader = PyPDFLoader(file_path, mode=\"single\")\n",
    "            docs = loader.load()\n",
    "\n",
    "            all_docs.append(docs[0]) \n",
    "            \n",
    "    return all_docs\n",
    "\n",
    "# Function to divide the extracted text into chunks\n",
    "def generate_chunks(all_docs):\n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(separator = \"\\n\",\n",
    "                                          chunk_size = 900, chunk_overlap = 100,\n",
    "                                          length_function = len)\n",
    "\n",
    "    chunks = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    return chunks\n",
    "    \n",
    "# Function to convert extract chunks into embeddings and store them in vector database\n",
    "def store_embeddings(chunks):\n",
    "    \n",
    "    embedding = HuggingFaceEmbeddings(model_name = \"NeuML/pubmedbert-base-embeddings\")\n",
    "    \n",
    "    # Setting up vector store\n",
    "    vstore = AstraDBVectorStore(embedding = embedding,\n",
    "                                collection_name = \"langchain_pdf_query\",\n",
    "                                api_endpoint = ASTRA_DB_API_ENDPOINT,\n",
    "                                token = ASTRA_DB_APPLICATION_TOKEN)\n",
    "\n",
    "    vstore.add_documents(chunks)\n",
    "    astra_vector_index = VectorStoreIndexWrapper(vectorstore = vstore)\n",
    "    \n",
    "    return vstore\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
