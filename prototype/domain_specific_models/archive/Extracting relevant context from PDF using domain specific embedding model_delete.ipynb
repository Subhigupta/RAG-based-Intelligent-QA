{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0915b7c",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30cd5162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b419b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c374e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f77dc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b308ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Astra DB \n",
    "ASTRA_DB_APPLICATION_TOKEN = os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"]\n",
    "ASTRA_DB_ID = os.environ[\"ASTRA_DB_ID\"]\n",
    "ASTRA_DB_API_ENDPOINT = os.environ[\"ASTRA_DB_API_ENDPOINT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ca0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make Hub downloads resilient on slower links\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"180\"\n",
    "os.environ[\"HF_HUB_DOWNLOAD_RETRY\"]   = \"20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d6680",
   "metadata": {},
   "source": [
    "### Read the research papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed927be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"D:\\Intelligent QA AI\\research_docs\"\n",
    "all_docs = []\n",
    "\n",
    "for file in os.listdir(dataset_path):\n",
    "    if file.endswith('.pdf'): \n",
    "        \n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "        loader = PyPDFLoader(file_path, mode=\"single\")\n",
    "        docs = loader.load()\n",
    "        \n",
    "        all_docs.append(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe20448",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ffd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = all_docs[0]\n",
    "doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = all_docs[1]\n",
    "doc.page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43358808",
   "metadata": {},
   "source": [
    "### Split the text into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47567d0b",
   "metadata": {},
   "source": [
    "#### Split using Recursive Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd94b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=900,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e481f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,4):\n",
    "    print(texts[i].page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb5df4",
   "metadata": {},
   "source": [
    "#### Split using Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78365fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=900,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "chunks = text_splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6735816",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad26c6e4",
   "metadata": {},
   "source": [
    "for i in range (0,9):\n",
    "    print(chunks[i].page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4cc85",
   "metadata": {},
   "source": [
    "### Creating Vector Embeddings & Storing Embeddings in a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d80861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     \"allenai/scibert_scivocab_uncased\",\n",
    "#     trust_remote_code=True,      # lets HF pick the safetensors file\n",
    "#     use_safetensors=True,        # force safe format\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8bf038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model = SentenceTransformer(\"NeuML/pubmedbert-base-embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf69f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"NeuML/pubmedbert-base-embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up vector store\n",
    "vstore = AstraDBVectorStore(\n",
    "    embedding=embedding,\n",
    "    collection_name=\"langchain_pdf_query\",\n",
    "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN\n",
    ")\n",
    "\n",
    "vstore.add_documents(chunks)\n",
    "astra_vector_index = VectorStoreIndexWrapper(vectorstore=vstore)\n",
    "\n",
    "print(\"Text chunks added to the vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2173b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is hybrid modeling approach?\"\n",
    "searchDocs = vstore.similarity_search(question, k=3)\n",
    "\n",
    "for i in range(len(searchDocs)):\n",
    "    print(searchDocs[i].page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8688c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the advantage of using hybrid modeling?\"\n",
    "searchDocs = vstore.similarity_search(question, k=3)\n",
    "\n",
    "for i in range(len(searchDocs)):\n",
    "    print(searchDocs[i].page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(searchDocs[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efa435",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = 0\n",
    "for i in range(len(searchDocs)):\n",
    "    tokens = tokenizer(searchDocs[i].page_content)\n",
    "    num_tokens = len(tokens['input_ids'])\n",
    "    total_tokens = total_tokens + num_tokens\n",
    "print(\"Number of tokens in input prompt:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-large\",\n",
    "    max_new_tokens=300,\n",
    "    min_length=150,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    temperature=1.2,\n",
    "    repetition_penalty=2.0,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    early_stopping=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n\".join([doc.page_content for doc in searchDocs])\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an expert in biopharmaceutical engineering.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Now answer the following question based on the above context, generate complete dinstictive lines:\n",
    "\n",
    "Q: {\"What is hybrid modeling?\"}\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "tokens = tokenizer(prompt)\n",
    "num_tokens = len(tokens['input_ids'])\n",
    "print(\"Number of tokens in input prompt:\", num_tokens)\n",
    "\n",
    "response = pipe(prompt)\n",
    "print(response[0]['generated_text'])\n",
    "\n",
    "response_tokens = tokenizer(response[0]['generated_text'])\n",
    "num_response_tokens = len(response_tokens['input_ids'])\n",
    "\n",
    "print(\"Number of tokens in the generated response:\", num_response_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd25d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "await vstore.aclear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81adc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380bc352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a43fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U auto-gptq optimum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c780e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"180\"      # seconds per read\n",
    "os.environ[\"HF_HUB_DOWNLOAD_RETRY\"]   = \"20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"TheBloke/PMC_LLAMA_7B-GPTQ\"   # 4-bit quantised checkpoint\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/PMC_LLAMA-7B-GPTQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae363a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/PMC_LLAMA-7B-GPTQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"chaoyi-wu/PMC_LLAMA_7B\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"chaoyi-wu/PMC_LLAMA_7B\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Add force_download=True to re-download the corrupted file\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f5cf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "C:\\Users\\subhi.gupta\\AppData\\Local\\anaconda3\\envs\\torchenv\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b6f97f1ab14e7480bb502a51327ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quantize_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subhi.gupta\\AppData\\Local\\anaconda3\\envs\\torchenv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\subhi.gupta\\.cache\\huggingface\\hub\\models--TheBloke--PMC_LLAMA-7B-GPTQ. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The following generation flags are not valid and may be ignored: ['pad_token_id']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['pad_token_id']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Some weights of the model checkpoint at C:\\Users\\subhi.gupta\\.cache\\huggingface\\hub\\models--TheBloke--PMC_LLAMA-7B-GPTQ\\snapshots\\7739ce0d4d7057bf5faf0efa19601dcd5640b346\\model.safetensors were not used when initializing LlamaForCausalLM: {'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq'}. This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM           # ← GPT-Q loader\n",
    "from transformers import AutoTokenizer\n",
    "import torch, os\n",
    "\n",
    "# make Hub downloads resilient on slower links\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"180\"\n",
    "os.environ[\"HF_HUB_DOWNLOAD_RETRY\"]   = \"20\"\n",
    "\n",
    "model_id  = \"TheBloke/PMC_LLAMA-7B-GPTQ\"         # dash, not underscore\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "            model_id,\n",
    "            device_map=\"auto\",      # fits layers into your 6 GB RTX 3050\n",
    "            use_safetensors=True,   # don’t trigger the torch-2.6 check\n",
    "            trust_remote_code=True  # repo has a custom loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a7fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
