{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0915b7c",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd5162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM  \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "from langchain_community.document_transformers.openai_functions import (\n",
    "    create_metadata_tagger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bed945",
   "metadata": {},
   "source": [
    "### Set the API token and other secret keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeb526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa6521",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_URI = os.environ[\"MONGODB_URI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab65412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make Hub downloads resilient on slower links\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"180\"\n",
    "os.environ[\"HF_HUB_DOWNLOAD_RETRY\"]   = \"20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MongoDB URI, DB, Collection Names\n",
    "\n",
    "client = MongoClient(MONGODB_URI)\n",
    "dbName = \"hybridModel_mongodb_chunks\"\n",
    "collectionName = \"chunked_data\"\n",
    "collection = client[dbName][collectionName]\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d6680",
   "metadata": {},
   "source": [
    "### Read the research papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed927be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"D:\\Intelligent QA AI\\research_docs\"\n",
    "all_docs = []\n",
    "\n",
    "for file in os.listdir(dataset_path):\n",
    "    if file.endswith('.pdf'): \n",
    "        \n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "        loader = PyPDFLoader(file_path, mode=\"single\")\n",
    "        docs = loader.load()\n",
    "        \n",
    "        if len(docs[0].page_content.split(\" \")) > 20: #avoiding storing empty pages \n",
    "            all_docs.append(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe20448",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ffd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = all_docs[0]\n",
    "doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714526",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43358808",
   "metadata": {},
   "source": [
    "### Split the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78365fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=900,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "chunks = text_splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6735816",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa74f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,9):\n",
    "    print(chunks[i].page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4cc85",
   "metadata": {},
   "source": [
    "### Creating Vector Embeddings & Storing Embeddings in a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf69f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"NeuML/pubmedbert-base-embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store = FAISS.from_documents(chunks, embedding)\n",
    "# vectorStore = MongoDBAtlasVectorSearch.from_documents(chunks, embedding, \n",
    "#                                                       collection=collection)\n",
    "\n",
    "# vectorStore = MongoDBAtlasVectorSearch.from_connection_string(MONGODB_URI,\n",
    "#     dbName + \".\" + collectionName,\n",
    "#     embedding,\n",
    "#     index_name=index,\n",
    "# )\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding,\n",
    "    collection=collection,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7963e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = MongoDBAtlasVectorSearch.from_connection_string(MONGODB_URI, dbName + \".\" + collectionName,\n",
    "                                                              embedding, index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88465278",
   "metadata": {},
   "source": [
    "### Find the similiar chunks from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171fcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is hybrid modeling approach?\"\n",
    "searchDocs = vector_store.similarity_search(question, k=3)\n",
    "# the query text is automatically embedded internally using the same embedding model you used to create your vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c285c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(searchDocs)):\n",
    "    print(searchDocs[i].page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60bd4e",
   "metadata": {},
   "source": [
    "### Load the tokenizer and count the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id  = \"TheBloke/PMC_LLAMA-7B-GPTQ\"         # dash, not underscore\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efa435",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = 0\n",
    "for i in range(len(searchDocs)):\n",
    "    tokens = tokenizer(searchDocs[i].page_content)\n",
    "    num_tokens = len(tokens['input_ids'])\n",
    "    total_tokens = total_tokens + num_tokens\n",
    "print(\"Number of tokens in input prompt:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3dce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tokenizer model max length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c2d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(\"TheBloke/PMC_LLAMA-7B-GPTQ\")\n",
    "\n",
    "# Check various token-related parameters\n",
    "print(f\"Max position embeddings: {config.max_position_embeddings}\")\n",
    "print(f\"Model max length: {getattr(config, 'max_length', 'Not specified')}\")\n",
    "print(f\"N positions: {getattr(config, 'n_positions', 'Not specified')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8cd555",
   "metadata": {},
   "source": [
    "### Load LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d01f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e270042",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./model_offload\", exist_ok=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    max_memory={0: \"5GB\", \"cpu\": \"14GB\"},  # Adjust based on your system\n",
    "    offload_folder=\"./model_offload\",\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ec180",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d651a",
   "metadata": {},
   "source": [
    "### Make inference through LLM by providing the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa791418",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n\".join([doc.page_content for doc in searchDocs])\n",
    "\n",
    "question = \"What is hybrid modeling?\"\n",
    "\n",
    "# Create the prompt\n",
    "prompt = f\"\"\"Based on the following context, please answer the question. Answer the question in descriptive way atleast in 4-5 lines.\n",
    "\n",
    "Context: {context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e8c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer\n",
    "#The truncation=True parameter acts as a safety mechanism that automatically cuts off your text if it exceeds \n",
    "# the specified max_length.\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f95e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be84e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move inputs to the same device as the model\n",
    "inputs = inputs.to(model.device)  # or inputs.to(\"cuda\") if you know it's on GPU\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    min_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    stop_strings=[\"\\n\\nQuestion:\", \"\\nQuestion:\", \"Question:\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Extract just the answer\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "answer = full_response[len(prompt):].strip()\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018f794",
   "metadata": {},
   "source": [
    "### Post-process the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop at various unwanted patterns\n",
    "stop_patterns = [\n",
    "    \"\\nContext:\",\n",
    "    \"\\nQuestion:\", \n",
    "    \"\\n\\nQuestion:\",\n",
    "    \"\\nQ:\",\n",
    "    \"Context:\",\n",
    "    \"Question:\",\n",
    "    \"\\n\\n\\n\"\n",
    "]\n",
    "\n",
    "for pattern in stop_patterns:\n",
    "    if pattern in answer:\n",
    "        answer = answer.split(pattern)[0].strip()\n",
    "        break\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014659d",
   "metadata": {},
   "source": [
    "### Chain of thought prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a50346",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Use the provided context to answer the question step-by-step.\n",
    "\n",
    "Context: {context_text}\n",
    "Question: {question}\n",
    "\n",
    "Think through this step by step:\n",
    "Step 1: What relevant information does the context provide?\n",
    "Step 2: What can we infer from this information?\n",
    "Step 3: What additional reasoning is needed?\n",
    "Step 4: What is the final answer?\n",
    "\n",
    "Step-by-step reasoning:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move inputs to the same device as the model\n",
    "inputs = inputs.to(model.device)  # or inputs.to(\"cuda\") if you know it's on GPU\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    min_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    stop_strings=[\"\\n\\nQuestion:\", \"\\nQuestion:\", \"Question:\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Extract just the answer\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "answer = full_response[len(prompt):].strip()\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c6db9",
   "metadata": {},
   "source": [
    "### BAsed on guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ce1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized for your PMC_LLAMA + MongoDB Atlas setup\n",
    "prompt = f\"\"\"As a medical AI assistant, analyze the following research context to provide evidence-based information.\n",
    "\n",
    "Retrieved Medical Literature:\n",
    "{context_text}\n",
    "\n",
    "Clinical Question: {question}\n",
    "\n",
    "Guidelines:\n",
    "- Base your response on the provided literature only\n",
    "- Cite specific studies or papers when available\n",
    "- If information is limited, state \"Based on the available literature...\"\n",
    "- Maintain clinical accuracy and appropriate caution\n",
    "\n",
    "Evidence-based Response:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17edfcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move inputs to the same device as the model\n",
    "inputs = inputs.to(model.device)  # or inputs.to(\"cuda\") if you know it's on GPU\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    min_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    stop_strings=[\"\\n\\nQuestion:\", \"\\nQuestion:\", \"Question:\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Extract just the answer\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "answer = full_response[len(prompt):].strip()\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "TASK: Analyze literature and provide evidence-based information.\n",
    "\n",
    "ROLE: You are a literature analyst with expertise in different techniques used in bio-pharama.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. READ the provided vontext carefully\n",
    "2. IDENTIFY relevant evidence that directly addresses the question\n",
    "3. ANALYZE the strength and quality of the evidence\n",
    "4. SYNTHESIZE findings into a coherent, useful response\n",
    "5. DO NOT add external knowledge or assumptions\n",
    "6. IF evidence is insufficient, state: \"The provided literature contains limited evidence for...\"\n",
    "\n",
    "INPUT:\n",
    "Medical Literature: {context_text}\n",
    "Clinical Question: {question}\n",
    "\n",
    "Begin your evidence-based analysis:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move inputs to the same device as the model\n",
    "inputs = inputs.to(model.device)  # or inputs.to(\"cuda\") if you know it's on GPU\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    min_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    stop_strings=[\"\\n\\nQuestion:\", \"\\nQuestion:\", \"Question:\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Extract just the answer\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "answer = full_response[len(prompt):].strip()\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607fd262",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3bfa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.delete(ids=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
